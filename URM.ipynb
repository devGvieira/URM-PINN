{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN for Uniform Rectilinear Motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Loss Function\n",
    "\n",
    "Below a Python function is developed to facilitate the development of our mathematical function, which afterwards will be used to define the network's loss function.   \n",
    "\n",
    "Given we are dealing with a Uniform Rectilinear Motion (URM), the differential equation is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\dfrac{dx}{dt} - v = 0,\n",
    "\\end{equation}\n",
    "\n",
    "with boundary condition given by\n",
    "\n",
    "\\begin{equation}\n",
    "    x(t=0) = 0\n",
    "\\end{equation}\n",
    "\n",
    "where $x$ refers to a temporal function of position, $t$ to time and $v$ to a constant velocity. Basically, this equation describes the movement of a body with constant velocity.\n",
    "\n",
    "For the purpose of this study, the domains considered will be\n",
    "\n",
    "\\begin{equation}\n",
    "    t \\in \\left[ 0,1 \\right]\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "    v = 0.5.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network's Architecture\n",
    "\n",
    "In the following cell, we define the archictecture's hyperparameters, note that the activation function applied here is a hyperbolic tangent (Tanh) for it being non-linear and ranging from -1 to 1. It is also interesting to pay attention to the fact that we are building the linear neural network from scratch due the lack of a PyTorch module specific for it, i.e., we are building a custom module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNN(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_inputs: int=1,\n",
    "            num_layers: int=1,\n",
    "            num_neurons: int=5,\n",
    "            act: nn.Module = nn.Tanh()\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(self.num_inputs, num_neurons))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers):\n",
    "            layers.extend([nn.Linear(num_neurons, num_neurons), act])\n",
    "\n",
    "        # Output layers\n",
    "        layers.append(nn.Linear(num_neurons, 1))\n",
    "\n",
    "        # Building the network as sequential\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    # Setting up the output\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x.reshape(-1,1)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the loss function for our network will be defined, we begin by defining the boundary conditions $x0 = x(t=0)$ and $v=0.5$, afterwards a tensor of 100 elements is defined to receive the possible temporal coordinates that the model may assume.\n",
    "\n",
    "Finally, the loss function will consider the model applied to the coordinates tensor and the function predicted at a given coordinate will be determined ($x_{pred}$). After determining the actual value of the function $x(t)$ at a given coordinate, the loss function will return the mean squared error between the predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random list of 100 temporal cordinates in 1D\n",
    "t = torch.rand(100, 1)\n",
    "\n",
    "# Position coordinate of a border\n",
    "x0 = 0\n",
    "# Velocity\n",
    "v = 0.5\n",
    "# Function at the border\n",
    "x = x0 + v*t\n",
    "\n",
    "# Loss in the border\n",
    "def loss_fn(model, t, x):\n",
    "    x_pred = model(t)\n",
    "    return torch.mean(torch.square(x - x_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with loss 0.28474152088165283\n",
      "Epoch 1 with loss 0.28474152088165283\n",
      "Epoch 2 with loss 0.28474152088165283\n",
      "Epoch 3 with loss 0.28474152088165283\n",
      "Epoch 4 with loss 0.28474152088165283\n",
      "Epoch 5 with loss 0.28474152088165283\n",
      "Epoch 6 with loss 0.28474152088165283\n",
      "Epoch 7 with loss 0.28474152088165283\n",
      "Epoch 8 with loss 0.28474152088165283\n",
      "Epoch 9 with loss 0.28474152088165283\n",
      "Epoch 10 with loss 0.28474152088165283\n",
      "Epoch 11 with loss 0.28474152088165283\n",
      "Epoch 12 with loss 0.28474152088165283\n",
      "Epoch 13 with loss 0.28474152088165283\n",
      "Epoch 14 with loss 0.28474152088165283\n",
      "Epoch 15 with loss 0.28474152088165283\n",
      "Epoch 16 with loss 0.28474152088165283\n",
      "Epoch 17 with loss 0.28474152088165283\n",
      "Epoch 18 with loss 0.28474152088165283\n",
      "Epoch 19 with loss 0.28474152088165283\n",
      "Epoch 20 with loss 0.28474152088165283\n",
      "Epoch 21 with loss 0.28474152088165283\n",
      "Epoch 22 with loss 0.28474152088165283\n",
      "Epoch 23 with loss 0.28474152088165283\n",
      "Epoch 24 with loss 0.28474152088165283\n",
      "Epoch 25 with loss 0.28474152088165283\n",
      "Epoch 26 with loss 0.28474152088165283\n",
      "Epoch 27 with loss 0.28474152088165283\n",
      "Epoch 28 with loss 0.28474152088165283\n",
      "Epoch 29 with loss 0.28474152088165283\n",
      "Epoch 30 with loss 0.28474152088165283\n",
      "Epoch 31 with loss 0.28474152088165283\n",
      "Epoch 32 with loss 0.28474152088165283\n",
      "Epoch 33 with loss 0.28474152088165283\n",
      "Epoch 34 with loss 0.28474152088165283\n",
      "Epoch 35 with loss 0.28474152088165283\n",
      "Epoch 36 with loss 0.28474152088165283\n",
      "Epoch 37 with loss 0.28474152088165283\n",
      "Epoch 38 with loss 0.28474152088165283\n",
      "Epoch 39 with loss 0.28474152088165283\n",
      "Epoch 40 with loss 0.28474152088165283\n",
      "Epoch 41 with loss 0.28474152088165283\n",
      "Epoch 42 with loss 0.28474152088165283\n",
      "Epoch 43 with loss 0.28474152088165283\n",
      "Epoch 44 with loss 0.28474152088165283\n",
      "Epoch 45 with loss 0.28474152088165283\n",
      "Epoch 46 with loss 0.28474152088165283\n",
      "Epoch 47 with loss 0.28474152088165283\n",
      "Epoch 48 with loss 0.28474152088165283\n",
      "Epoch 49 with loss 0.28474152088165283\n",
      "Epoch 50 with loss 0.28474152088165283\n",
      "Epoch 51 with loss 0.28474152088165283\n",
      "Epoch 52 with loss 0.28474152088165283\n",
      "Epoch 53 with loss 0.28474152088165283\n",
      "Epoch 54 with loss 0.28474152088165283\n",
      "Epoch 55 with loss 0.28474152088165283\n",
      "Epoch 56 with loss 0.28474152088165283\n",
      "Epoch 57 with loss 0.28474152088165283\n",
      "Epoch 58 with loss 0.28474152088165283\n",
      "Epoch 59 with loss 0.28474152088165283\n",
      "Epoch 60 with loss 0.28474152088165283\n",
      "Epoch 61 with loss 0.28474152088165283\n",
      "Epoch 62 with loss 0.28474152088165283\n",
      "Epoch 63 with loss 0.28474152088165283\n",
      "Epoch 64 with loss 0.28474152088165283\n",
      "Epoch 65 with loss 0.28474152088165283\n",
      "Epoch 66 with loss 0.28474152088165283\n",
      "Epoch 67 with loss 0.28474152088165283\n",
      "Epoch 68 with loss 0.28474152088165283\n",
      "Epoch 69 with loss 0.28474152088165283\n",
      "Epoch 70 with loss 0.28474152088165283\n",
      "Epoch 71 with loss 0.28474152088165283\n",
      "Epoch 72 with loss 0.28474152088165283\n",
      "Epoch 73 with loss 0.28474152088165283\n",
      "Epoch 74 with loss 0.28474152088165283\n",
      "Epoch 75 with loss 0.28474152088165283\n",
      "Epoch 76 with loss 0.28474152088165283\n",
      "Epoch 77 with loss 0.28474152088165283\n",
      "Epoch 78 with loss 0.28474152088165283\n",
      "Epoch 79 with loss 0.28474152088165283\n",
      "Epoch 80 with loss 0.28474152088165283\n",
      "Epoch 81 with loss 0.28474152088165283\n",
      "Epoch 82 with loss 0.28474152088165283\n",
      "Epoch 83 with loss 0.28474152088165283\n",
      "Epoch 84 with loss 0.28474152088165283\n",
      "Epoch 85 with loss 0.28474152088165283\n",
      "Epoch 86 with loss 0.28474152088165283\n",
      "Epoch 87 with loss 0.28474152088165283\n",
      "Epoch 88 with loss 0.28474152088165283\n",
      "Epoch 89 with loss 0.28474152088165283\n",
      "Epoch 90 with loss 0.28474152088165283\n",
      "Epoch 91 with loss 0.28474152088165283\n",
      "Epoch 92 with loss 0.28474152088165283\n",
      "Epoch 93 with loss 0.28474152088165283\n",
      "Epoch 94 with loss 0.28474152088165283\n",
      "Epoch 95 with loss 0.28474152088165283\n",
      "Epoch 96 with loss 0.28474152088165283\n",
      "Epoch 97 with loss 0.28474152088165283\n",
      "Epoch 98 with loss 0.28474152088165283\n",
      "Epoch 99 with loss 0.28474152088165283\n"
     ]
    }
   ],
   "source": [
    "# Running the structure created\n",
    "model = LinearNN()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = loss_fn(model, t, x)\n",
    "    loss.backward()\n",
    "\n",
    "    print(f\"Epoch {epoch} with loss {float(loss)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vmap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m functional_call(model, params, (x, ))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Mathematical function\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m dsdx \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m(grad(f), in_dims\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vmap' is not defined"
     ]
    }
   ],
   "source": [
    "# Running the structure created\n",
    "model = LinearNN()\n",
    "\n",
    "# Python function to build mathematical functions\n",
    "def f(x: torch.Tensor, params: dict({str: torch.nn.Parameter} )) -> torch.Tensor:\n",
    "    return functional_call(model, params, (x, ))\n",
    "\n",
    "# Mathematical function\n",
    "dsdx = vmap(grad(f), in_dims=(0, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the boundaries values\n",
    "## Space boundary\n",
    "S_BOUNDARY = 0.0\n",
    "## Velocity boundary\n",
    "C_BOUNDARY = 0.5\n",
    "\n",
    "def loss_fn(params: torch.Tensor, x: torch.Tensor):\n",
    "\n",
    "    # Interior equation\n",
    "    interior = dsdx(x, params) - C_BOUNDARY\n",
    "    \n",
    "    x0 = S_BOUNDARY\n",
    "    c0 = C_BOUNDARY\n",
    "    # Parametrizing boundary\n",
    "    s_boundary = torch.tensor([x0])\n",
    "    c_boundary = torch.tensor([c0])\n",
    "    \n",
    "    # Boundary equation\n",
    "    boundary = f(s_boundary, params) - c_boundary \n",
    "\n",
    "    # Setting Mean Squared Error as metric for loss\n",
    "    loss = nn.MSELoss()\n",
    "    # Getting numerical value for loss\n",
    "    loss_value = loss(interior, torch.zeros_like(interior)) + loss(boundary, torch.zeros_like(boundary))\n",
    "\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected all elements of parameter_and_buffer_dicts to be dictionaries",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iter):\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(batch_size)\u001b[38;5;241m.\u001b[39muniform_(domain[\u001b[38;5;241m0\u001b[39m], domain[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     params \u001b[38;5;241m=\u001b[39m optmizer\u001b[38;5;241m.\u001b[39mstep(loss, params)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mfloat\u001b[39m(loss)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(params, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(params: torch\u001b[38;5;241m.\u001b[39mTensor, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Interior equation\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     interior \u001b[38;5;241m=\u001b[39m \u001b[43mdsdx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m C_BOUNDARY\n\u001b[1;32m     12\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m S_BOUNDARY\n\u001b[1;32m     13\u001b[0m     c0 \u001b[38;5;241m=\u001b[39m C_BOUNDARY\n",
      "File \u001b[0;32m~/Documents/URM-PINN/URM-venv/lib/python3.11/site-packages/torch/_functorch/apis.py:188\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/URM-PINN/URM-venv/lib/python3.11/site-packages/torch/_functorch/vmap.py:266\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[1;32m    263\u001b[0m                          args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/URM-PINN/URM-venv/lib/python3.11/site-packages/torch/_functorch/vmap.py:38\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/URM-PINN/URM-venv/lib/python3.11/site-packages/torch/_functorch/vmap.py:379\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 379\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/URM-PINN/URM-venv/lib/python3.11/site-packages/torch/_functorch/apis.py:363\u001b[0m, in \u001b[0;36mgrad.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meager_transforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/URM-PINN/URM-venv/lib/python3.11/site-packages/torch/_functorch/eager_transforms.py:1287\u001b[0m, in \u001b[0;36mgrad_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_impl\u001b[39m(func: Callable, argnums: argnums_t, has_aux: \u001b[38;5;28mbool\u001b[39m, args, kwargs):\n\u001b[1;32m   1286\u001b[0m     func \u001b[38;5;241m=\u001b[39m lazy_dynamo_disable(func)\n\u001b[0;32m-> 1287\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_and_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1289\u001b[0m         grad, (_, aux) \u001b[38;5;241m=\u001b[39m results\n",
      "File \u001b[0;32m~/Documents/URM-PINN/URM-venv/lib/python3.11/site-packages/torch/_functorch/vmap.py:38\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/URM-PINN/URM-venv/lib/python3.11/site-packages/torch/_functorch/eager_transforms.py:1248\u001b[0m, in \u001b[0;36mgrad_and_value.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1245\u001b[0m diff_args \u001b[38;5;241m=\u001b[39m _slice_argnums(args, argnums, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1246\u001b[0m tree_map_(partial(_create_differentiable, level\u001b[38;5;241m=\u001b[39mlevel), diff_args)\n\u001b[0;32m-> 1248\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/URM-PINN/URM-venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m, in \u001b[0;36mf\u001b[0;34m(x, params)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x: torch\u001b[38;5;241m.\u001b[39mTensor, params: \u001b[38;5;28mdict\u001b[39m({\u001b[38;5;28mstr\u001b[39m: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter} )) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/URM-PINN/URM-venv/lib/python3.11/site-packages/torch/_functorch/functional_call.py:125\u001b[0m, in \u001b[0;36mfunctional_call\u001b[0;34m(module, parameter_and_buffer_dicts, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parameter_and_buffer_dicts, Sequence):\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(d, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m parameter_and_buffer_dicts):\n\u001b[0;32m--> 125\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    126\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all elements of parameter_and_buffer_dicts to be dictionaries\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m         )\n\u001b[1;32m    128\u001b[0m     all_keys \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m parameter_and_buffer_dicts \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m d\u001b[38;5;241m.\u001b[39mkeys()]\n\u001b[1;32m    129\u001b[0m     repeated_keys \u001b[38;5;241m=\u001b[39m [key \u001b[38;5;28;01mfor\u001b[39;00m key, n \u001b[38;5;129;01min\u001b[39;00m Counter(all_keys)\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Expected all elements of parameter_and_buffer_dicts to be dictionaries"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "num_iter = 100\n",
    "learning_rate = 1e-1\n",
    "domain = (-5.0,5.0)\n",
    "\n",
    "optmizer = torchopt.FuncOptimizer(torchopt.adam(lr=learning_rate))\n",
    "\n",
    "params = tuple(model.parameters())\n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    x = torch.FloatTensor(batch_size).uniform_(domain[0], domain[1])\n",
    "\n",
    "    loss = loss_fn(params, x)\n",
    "\n",
    "    params = optmizer.step(loss, params)\n",
    "\n",
    "    print(f\"Iteration {i} with loss {float(loss)}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UARM-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
