{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN for Uniform Rectilinear Motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Loss Function\n",
    "\n",
    "Below a Python function is developed to facilitate the development of our mathematical function, which afterwards will be used to define the network's loss function.   \n",
    "\n",
    "Given we are dealing with a Uniform Rectilinear Motion (URM), the differential equation is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\dfrac{dx}{dt} - v = 0,\n",
    "\\end{equation}\n",
    "\n",
    "with boundary condition given by\n",
    "\n",
    "\\begin{equation}\n",
    "    x(t=0) = 0\n",
    "\\end{equation}\n",
    "\n",
    "where $x$ refers to a temporal function of position, $t$ to time and $v$ to a constant velocity. Basically, this equation describes the movement of a body with constant velocity.\n",
    "\n",
    "For the purpose of this study, the domains considered will be\n",
    "\n",
    "\\begin{equation}\n",
    "    t \\in \\left[ 0,1 \\right]\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "    v = 0.5.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network's Architecture\n",
    "\n",
    "In the following cell, we define the archictecture's hyperparameters, note that the activation function applied here is a hyperbolic tangent (Tanh) for it being non-linear and ranging from -1 to 1. It is also interesting to pay attention to the fact that we are building the linear neural network from scratch due the lack of a PyTorch module specific for it, i.e., we are building a custom module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNN(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_inputs: int=1,\n",
    "            num_layers: int=1,\n",
    "            num_neurons: int=5,\n",
    "            act: nn.Module = nn.Tanh()\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(self.num_inputs, num_neurons))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers):\n",
    "            layers.extend([nn.Linear(num_neurons, num_neurons), act])\n",
    "\n",
    "        # Output layers\n",
    "        layers.append(nn.Linear(num_neurons, 1))\n",
    "\n",
    "        # Building the network as sequential\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    # Setting up the output\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x.reshape(-1,1)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the loss function for our network will be defined, we begin by defining the boundary conditions $x0 = x(t=0)$ and $v=0.5$, afterwards a tensor of 100 elements is defined to receive the possible temporal coordinates that the model may assume.\n",
    "\n",
    "Finally, the loss function will consider the model applied to the coordinates tensor and the function predicted at a given coordinate will be determined ($x_{pred}$). After determining the actual value of the function $x(t)$ at a given coordinate, the loss function will return the mean squared error between the predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random list of 100 temporal cordinates in 1D\n",
    "t = torch.rand(100, 1)\n",
    "\n",
    "# Position coordinate of a border\n",
    "x0 = 0\n",
    "# Velocity\n",
    "v = 0.5\n",
    "# Function at the border\n",
    "x = x0 + v*t\n",
    "\n",
    "# Loss in the border\n",
    "def loss_fn(model, t, x):\n",
    "    x_pred = model(t)\n",
    "    return torch.mean(torch.square(x - x_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with loss 0.28474152088165283\n",
      "Epoch 1 with loss 0.28474152088165283\n",
      "Epoch 2 with loss 0.28474152088165283\n",
      "Epoch 3 with loss 0.28474152088165283\n",
      "Epoch 4 with loss 0.28474152088165283\n",
      "Epoch 5 with loss 0.28474152088165283\n",
      "Epoch 6 with loss 0.28474152088165283\n",
      "Epoch 7 with loss 0.28474152088165283\n",
      "Epoch 8 with loss 0.28474152088165283\n",
      "Epoch 9 with loss 0.28474152088165283\n",
      "Epoch 10 with loss 0.28474152088165283\n",
      "Epoch 11 with loss 0.28474152088165283\n",
      "Epoch 12 with loss 0.28474152088165283\n",
      "Epoch 13 with loss 0.28474152088165283\n",
      "Epoch 14 with loss 0.28474152088165283\n",
      "Epoch 15 with loss 0.28474152088165283\n",
      "Epoch 16 with loss 0.28474152088165283\n",
      "Epoch 17 with loss 0.28474152088165283\n",
      "Epoch 18 with loss 0.28474152088165283\n",
      "Epoch 19 with loss 0.28474152088165283\n",
      "Epoch 20 with loss 0.28474152088165283\n",
      "Epoch 21 with loss 0.28474152088165283\n",
      "Epoch 22 with loss 0.28474152088165283\n",
      "Epoch 23 with loss 0.28474152088165283\n",
      "Epoch 24 with loss 0.28474152088165283\n",
      "Epoch 25 with loss 0.28474152088165283\n",
      "Epoch 26 with loss 0.28474152088165283\n",
      "Epoch 27 with loss 0.28474152088165283\n",
      "Epoch 28 with loss 0.28474152088165283\n",
      "Epoch 29 with loss 0.28474152088165283\n",
      "Epoch 30 with loss 0.28474152088165283\n",
      "Epoch 31 with loss 0.28474152088165283\n",
      "Epoch 32 with loss 0.28474152088165283\n",
      "Epoch 33 with loss 0.28474152088165283\n",
      "Epoch 34 with loss 0.28474152088165283\n",
      "Epoch 35 with loss 0.28474152088165283\n",
      "Epoch 36 with loss 0.28474152088165283\n",
      "Epoch 37 with loss 0.28474152088165283\n",
      "Epoch 38 with loss 0.28474152088165283\n",
      "Epoch 39 with loss 0.28474152088165283\n",
      "Epoch 40 with loss 0.28474152088165283\n",
      "Epoch 41 with loss 0.28474152088165283\n",
      "Epoch 42 with loss 0.28474152088165283\n",
      "Epoch 43 with loss 0.28474152088165283\n",
      "Epoch 44 with loss 0.28474152088165283\n",
      "Epoch 45 with loss 0.28474152088165283\n",
      "Epoch 46 with loss 0.28474152088165283\n",
      "Epoch 47 with loss 0.28474152088165283\n",
      "Epoch 48 with loss 0.28474152088165283\n",
      "Epoch 49 with loss 0.28474152088165283\n",
      "Epoch 50 with loss 0.28474152088165283\n",
      "Epoch 51 with loss 0.28474152088165283\n",
      "Epoch 52 with loss 0.28474152088165283\n",
      "Epoch 53 with loss 0.28474152088165283\n",
      "Epoch 54 with loss 0.28474152088165283\n",
      "Epoch 55 with loss 0.28474152088165283\n",
      "Epoch 56 with loss 0.28474152088165283\n",
      "Epoch 57 with loss 0.28474152088165283\n",
      "Epoch 58 with loss 0.28474152088165283\n",
      "Epoch 59 with loss 0.28474152088165283\n",
      "Epoch 60 with loss 0.28474152088165283\n",
      "Epoch 61 with loss 0.28474152088165283\n",
      "Epoch 62 with loss 0.28474152088165283\n",
      "Epoch 63 with loss 0.28474152088165283\n",
      "Epoch 64 with loss 0.28474152088165283\n",
      "Epoch 65 with loss 0.28474152088165283\n",
      "Epoch 66 with loss 0.28474152088165283\n",
      "Epoch 67 with loss 0.28474152088165283\n",
      "Epoch 68 with loss 0.28474152088165283\n",
      "Epoch 69 with loss 0.28474152088165283\n",
      "Epoch 70 with loss 0.28474152088165283\n",
      "Epoch 71 with loss 0.28474152088165283\n",
      "Epoch 72 with loss 0.28474152088165283\n",
      "Epoch 73 with loss 0.28474152088165283\n",
      "Epoch 74 with loss 0.28474152088165283\n",
      "Epoch 75 with loss 0.28474152088165283\n",
      "Epoch 76 with loss 0.28474152088165283\n",
      "Epoch 77 with loss 0.28474152088165283\n",
      "Epoch 78 with loss 0.28474152088165283\n",
      "Epoch 79 with loss 0.28474152088165283\n",
      "Epoch 80 with loss 0.28474152088165283\n",
      "Epoch 81 with loss 0.28474152088165283\n",
      "Epoch 82 with loss 0.28474152088165283\n",
      "Epoch 83 with loss 0.28474152088165283\n",
      "Epoch 84 with loss 0.28474152088165283\n",
      "Epoch 85 with loss 0.28474152088165283\n",
      "Epoch 86 with loss 0.28474152088165283\n",
      "Epoch 87 with loss 0.28474152088165283\n",
      "Epoch 88 with loss 0.28474152088165283\n",
      "Epoch 89 with loss 0.28474152088165283\n",
      "Epoch 90 with loss 0.28474152088165283\n",
      "Epoch 91 with loss 0.28474152088165283\n",
      "Epoch 92 with loss 0.28474152088165283\n",
      "Epoch 93 with loss 0.28474152088165283\n",
      "Epoch 94 with loss 0.28474152088165283\n",
      "Epoch 95 with loss 0.28474152088165283\n",
      "Epoch 96 with loss 0.28474152088165283\n",
      "Epoch 97 with loss 0.28474152088165283\n",
      "Epoch 98 with loss 0.28474152088165283\n",
      "Epoch 99 with loss 0.28474152088165283\n"
     ]
    }
   ],
   "source": [
    "# Running the structure created\n",
    "model = LinearNN()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = loss_fn(model, t, x)\n",
    "    loss.backward()\n",
    "\n",
    "    print(f\"Epoch {epoch} with loss {float(loss)}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UARM-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
